{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Pipeline for Speaker Separation and Identification\n",
    "The final part of the assignment involves designing and evaluating a novel pipeline that combines speaker separation and identification into a unified system, trained and tested on the custom multi-speaker dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T14:31:43.248791Z",
     "iopub.status.busy": "2025-04-03T14:31:43.248542Z",
     "iopub.status.idle": "2025-04-03T14:31:49.036700Z",
     "shell.execute_reply": "2025-04-03T14:31:49.035841Z",
     "shell.execute_reply.started": "2025-04-03T14:31:43.248770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:31:49.037915Z",
     "iopub.status.busy": "2025-04-03T14:31:49.037675Z",
     "iopub.status.idle": "2025-04-03T14:32:00.546515Z",
     "shell.execute_reply": "2025-04-03T14:32:00.545623Z",
     "shell.execute_reply.started": "2025-04-03T14:31:49.037894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pesq in ./sr_venv/lib/python3.8/site-packages (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pesq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:00.547680Z",
     "iopub.status.busy": "2025-04-03T14:32:00.547445Z",
     "iopub.status.idle": "2025-04-03T14:32:04.134402Z",
     "shell.execute_reply": "2025-04-03T14:32:04.133519Z",
     "shell.execute_reply.started": "2025-04-03T14:32:00.547659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mir-eval in ./sr_venv/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./sr_venv/lib/python3.8/site-packages (from mir-eval) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.4.0 in ./sr_venv/lib/python3.8/site-packages (from mir-eval) (1.10.1)\n",
      "Requirement already satisfied: decorator in ./sr_venv/lib/python3.8/site-packages (from mir-eval) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mir-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:04.136619Z",
     "iopub.status.busy": "2025-04-03T14:32:04.136385Z",
     "iopub.status.idle": "2025-04-03T14:32:24.525903Z",
     "shell.execute_reply": "2025-04-03T14:32:24.525249Z",
     "shell.execute_reply.started": "2025-04-03T14:32:04.136597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gr1/himanshu/voxCeleb/sr_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/tmp/ipykernel_3520595/4011597208.py:8: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SepformerSeparation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from speechbrain.pretrained import SepformerSeparation\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "from pesq import pesq\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "import torch.nn.functional as F\n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import torchaudio.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3520595/969504665.py:1: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    }
   ],
   "source": [
    "torchaudio.set_audio_backend(\"ffmpeg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.527556Z",
     "iopub.status.busy": "2025-04-03T14:32:24.526987Z",
     "iopub.status.idle": "2025-04-03T14:32:24.532251Z",
     "shell.execute_reply": "2025-04-03T14:32:24.531451Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.527523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.533303Z",
     "iopub.status.busy": "2025-04-03T14:32:24.533048Z",
     "iopub.status.idle": "2025-04-03T14:32:24.567522Z",
     "shell.execute_reply": "2025-04-03T14:32:24.566693Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.533283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VOXCELEB2_TXT_BASE_DIR = \"Dataset/vox2_test_txt/txt\"\n",
    "VOXCELEB2_AUDIO_BASE_DIR = \"Converted_WAVs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.568547Z",
     "iopub.status.busy": "2025-04-03T14:32:24.568263Z",
     "iopub.status.idle": "2025-04-03T14:32:24.582344Z",
     "shell.execute_reply": "2025-04-03T14:32:24.581536Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.568525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_voxceleb_metadata(txt_base_dir, audio_base_dir):\n",
    "    \n",
    "    speaker_dict = {}\n",
    "    pattern = os.path.join(txt_base_dir, \"**\", \"*.txt\")\n",
    "    txt_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        rel_path = os.path.relpath(txt_file, txt_base_dir)\n",
    "        parts = rel_path.split(os.sep)\n",
    "        if len(parts) < 3:\n",
    "            continue  \n",
    "        \n",
    "        speaker_id = parts[0]\n",
    "        recording_id = parts[1]\n",
    "        file_name = parts[2]\n",
    "        audio_file = os.path.join(audio_base_dir, speaker_id, recording_id, file_name.replace('.txt', '.wav'))\n",
    "        if not os.path.exists(audio_file):\n",
    "            continue\n",
    "        \n",
    "        if speaker_id not in speaker_dict:\n",
    "            speaker_dict[speaker_id] = []\n",
    "        speaker_dict[speaker_id].append(audio_file)\n",
    "    \n",
    "    return speaker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.583506Z",
     "iopub.status.busy": "2025-04-03T14:32:24.583170Z",
     "iopub.status.idle": "2025-04-03T14:32:24.597622Z",
     "shell.execute_reply": "2025-04-03T14:32:24.596815Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.583472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio_file(file_path, target_sr):\n",
    "    audio, sr = torchaudio.load(file_path)\n",
    "    if sr != target_sr:\n",
    "        transform = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        audio = transform(audio)\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = audio.mean(dim=0, keepdim=True)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.598697Z",
     "iopub.status.busy": "2025-04-03T14:32:24.598389Z",
     "iopub.status.idle": "2025-04-03T14:32:24.618133Z",
     "shell.execute_reply": "2025-04-03T14:32:24.617353Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.598664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_or_truncate(aud1, aud2):\n",
    "    len1 = aud1.shape[1]\n",
    "    len2 = aud2.shape[1]\n",
    "    if len1 < len2:\n",
    "        pad = torch.zeros(1, len2 - len1)\n",
    "        aud1 = torch.cat([aud1, pad], dim=1)\n",
    "    elif len2 < len1:\n",
    "        pad = torch.zeros(1, len1 - len2)\n",
    "        aud2 = torch.cat([aud2, pad], dim=1)\n",
    "    return aud1, aud2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.619271Z",
     "iopub.status.busy": "2025-04-03T14:32:24.618972Z",
     "iopub.status.idle": "2025-04-03T14:32:24.634148Z",
     "shell.execute_reply": "2025-04-03T14:32:24.633530Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.619243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mix_utterances(utt1, utt2, snr_dB=0):\n",
    "    utt1, utt2 = pad_or_truncate(utt1, utt2)\n",
    "    power1 = utt1.pow(2).mean()\n",
    "    power2 = utt2.pow(2).mean()\n",
    "    scale = torch.sqrt(power1 / (10**(snr_dB/10) * power2 + 1e-8))\n",
    "    utt2_scaled = utt2 * scale\n",
    "    mixture = utt1 + utt2_scaled\n",
    "    return mixture, utt1, utt2_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIXED_DURATION = 3  \n",
    "\n",
    "import torch\n",
    "\n",
    "def fix_audio_length(audio, sr, target_duration=3):\n",
    "    target_length = int(target_duration * sr)  \n",
    "    audio_length = len(audio)\n",
    "\n",
    "    if audio_length > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    elif audio_length < target_length:\n",
    "        pad_length = target_length - audio_length\n",
    "        audio = np.pad(audio, (0, pad_length), mode='constant')\n",
    "\n",
    "    return torch.tensor(audio, dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.635256Z",
     "iopub.status.busy": "2025-04-03T14:32:24.634987Z",
     "iopub.status.idle": "2025-04-03T14:32:24.648489Z",
     "shell.execute_reply": "2025-04-03T14:32:24.647874Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.635224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_scenario(speaker_dict, speaker_ids, num_mixtures=1000, target_sr=16000):\n",
    "    \n",
    "    mixtures = []\n",
    "    available_speakers = [s for s in speaker_ids if s in speaker_dict and len(speaker_dict[s]) > 0]\n",
    "    if len(available_speakers) < 2:\n",
    "        raise ValueError(\"Need at least two speakers to create mixtures.\")\n",
    "    \n",
    "    for _ in range(num_mixtures):\n",
    "        spk1, spk2 = random.sample(available_speakers, 2)\n",
    "        utt1_path = random.choice(speaker_dict[spk1])\n",
    "        utt2_path = random.choice(speaker_dict[spk2])\n",
    "        utt1 = load_audio_file(utt1_path, target_sr)\n",
    "        utt2 = load_audio_file(utt2_path, target_sr)\n",
    "        utt1 = fix_audio_length(utt1, target_sr, FIXED_DURATION)\n",
    "        utt2 = fix_audio_length(utt2, target_sr, FIXED_DURATION)\n",
    "        mixture, ref1, ref2 = mix_utterances(utt1, utt2, snr_dB=0)\n",
    "        mixture = fix_audio_length(mixture, target_sr, FIXED_DURATION)\n",
    "        mixtures.append((mixture, [ref1, ref2], [spk1, spk2]))\n",
    "    return mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:39:25.754148Z",
     "iopub.status.busy": "2025-04-03T14:39:25.753853Z",
     "iopub.status.idle": "2025-04-03T14:39:25.761619Z",
     "shell.execute_reply": "2025-04-03T14:39:25.760871Z",
     "shell.execute_reply.started": "2025-04-03T14:39:25.754126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class JointSeparationIdentification(nn.Module):\n",
    "    def __init__(self, sepformer_model, id_model, num_speakers, embedding_dim, id_loss_weight=0.5):\n",
    "        super(JointSeparationIdentification, self).__init__()\n",
    "        self.sepformer = sepformer_model  \n",
    "        for param in self.sepformer.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.id_model = id_model  \n",
    "        for layer in self.id_model.encoder.layers[:10]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.classifier = nn.Linear(embedding_dim, num_speakers)\n",
    "        self.id_loss_weight = id_loss_weight\n",
    "        self.resampler = T.Resample(orig_freq=8000, new_freq=16000)\n",
    "    \n",
    "    def forward(self, mixture, labels=None): \n",
    "        mixture_input = mixture.squeeze(1)  \n",
    "        separated = self.sepformer.separate_batch(mixture_input)\n",
    "        separated = separated.squeeze(1).transpose(1, 2)\n",
    "    \n",
    "        B, n, T = separated.shape\n",
    "        separated_16k = []\n",
    "        for i in range(n):\n",
    "            src = separated[:, i, :].unsqueeze(1)  \n",
    "            upsampled = self.resampler(src)         \n",
    "            separated_16k.append(upsampled.squeeze(1))\n",
    "        separated_16k = torch.stack(separated_16k, dim=1)  \n",
    "        embeddings = []\n",
    "        for i in range(n):\n",
    "            source_i = separated_16k[:, i, :]  \n",
    "            id_outputs = self.id_model(source_i)\n",
    "            emb = id_outputs.last_hidden_state.mean(dim=1)  \n",
    "            embeddings.append(emb)\n",
    "        embeddings = torch.stack(embeddings, dim=1)  \n",
    "\n",
    "        if labels is not None:\n",
    "            B, n, emb_dim = embeddings.shape\n",
    "            flat_embeddings = embeddings.reshape(-1, emb_dim)  \n",
    "            logits = self.classifier(flat_embeddings)  \n",
    "            logits = logits.view(B, n, -1)  \n",
    "            return separated_16k, embeddings, logits\n",
    "        else:\n",
    "            return separated_16k, embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.668358Z",
     "iopub.status.busy": "2025-04-03T14:32:24.668122Z",
     "iopub.status.idle": "2025-04-03T14:32:24.682756Z",
     "shell.execute_reply": "2025-04-03T14:32:24.681998Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.668339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def joint_loss(separated, references, logits, labels, id_loss_weight=0.5):\n",
    "    min_length = min(separated.size(2), references.size(2))\n",
    "    separated = separated[:, :, :min_length]\n",
    "    references = references[:, :, :min_length]\n",
    "    sep_loss = F.mse_loss(separated, references)\n",
    "    B, n, _ = logits.shape\n",
    "    id_loss = F.cross_entropy(logits.view(B * n, -1), labels.view(B * n))\n",
    "    \n",
    "    return sep_loss + id_loss_weight * id_loss, sep_loss, id_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.684804Z",
     "iopub.status.busy": "2025-04-03T14:32:24.684557Z",
     "iopub.status.idle": "2025-04-03T14:32:24.697567Z",
     "shell.execute_reply": "2025-04-03T14:32:24.696755Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.684767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(ref_sources, est_sources, sample_rate):\n",
    "    SDR, SIR, SAR, _ = bss_eval_sources(ref_sources, est_sources)\n",
    "    pesq_mode = \"nb\" if sample_rate == 8000 else \"wb\"\n",
    "\n",
    "    pesq_scores = []\n",
    "    for i in range(ref_sources.shape[0]):\n",
    "        score = pesq(sample_rate, ref_sources[i], est_sources[i], mode=pesq_mode)\n",
    "        pesq_scores.append(score)\n",
    "\n",
    "    return SDR, SIR, SAR, pesq_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:37:30.813589Z",
     "iopub.status.busy": "2025-04-03T14:37:30.813284Z",
     "iopub.status.idle": "2025-04-03T14:37:30.819332Z",
     "shell.execute_reply": "2025-04-03T14:37:30.818555Z",
     "shell.execute_reply.started": "2025-04-03T14:37:30.813567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_joint_model(joint_model, train_loader, optimizer, device, num_epochs):\n",
    "    joint_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for mixtures, references, speaker_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            mixtures = mixtures.to(device)\n",
    "            references = references.to(device)\n",
    "            speaker_labels = speaker_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                separated, embeddings, logits = joint_model(mixtures, speaker_labels)\n",
    "                loss, sep_loss, id_loss = joint_loss(separated, references, logits, speaker_labels, joint_model.id_loss_weight)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_joint_model(joint_model, test_loader, device):\n",
    "    joint_model.eval()\n",
    "    sep_metrics = []\n",
    "\n",
    "    def best_permutation_match(ref_sources, est_sources, sample_rate):\n",
    "        best_sdr = -float('inf')\n",
    "        best_perm = None\n",
    "        for perm in permutations([0, 1]):\n",
    "            perm_est_sources = est_sources[list(perm), :]\n",
    "            SDR, _, _, _ = evaluate_metrics(ref_sources, perm_est_sources, sample_rate)\n",
    "            total_sdr = sum(SDR)\n",
    "            if total_sdr > best_sdr:\n",
    "                best_sdr = total_sdr\n",
    "                best_perm = perm\n",
    "        return best_perm\n",
    "\n",
    "    \n",
    "    for mixtures, references, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        mixtures = mixtures.to(device)\n",
    "        separated, _ = joint_model(mixtures) \n",
    "        B, n, T = separated.shape\n",
    "\n",
    "        for i in range(B):\n",
    "            ref_sources = np.array([r.squeeze().cpu().numpy() for r in references[i]])  \n",
    "            est_sources = np.array([\n",
    "                librosa.resample(src, orig_sr=16000, target_sr=int(16000 * (ref_sources.shape[1] / src.shape[0])))\n",
    "                for src in separated[i].detach().cpu().numpy()\n",
    "                ])\n",
    "            best_perm = best_permutation_match(ref_sources, est_sources, 16000)\n",
    "            est_sources = est_sources[list(best_perm), :]\n",
    "            SDR, SIR, SAR, pesq_scores = evaluate_metrics(ref_sources, est_sources, 16000)\n",
    "            sep_metrics.append({\n",
    "                \"SDR\": SDR,\n",
    "                \"SIR\": SIR,\n",
    "                \"SAR\": SAR,\n",
    "                \"PESQ\": pesq_scores\n",
    "            })\n",
    "\n",
    "    avg_SDR = np.mean([m['SDR'][0] for m in sep_metrics])\n",
    "    avg_SIR = np.mean([m['SIR'][0] for m in sep_metrics])\n",
    "    avg_SAR = np.mean([m['SAR'][0] for m in sep_metrics])\n",
    "    avg_PESQ = np.mean([m['PESQ'][0] for m in sep_metrics])\n",
    "\n",
    "    print(\"\\n--- Test Separation Metrics ---\")\n",
    "    print(f\"Average SDR: {avg_SDR:.2f} dB\")\n",
    "    print(f\"Average SIR: {avg_SIR:.2f} dB\")\n",
    "    print(f\"Average SAR: {avg_SAR:.2f} dB\")\n",
    "    print(f\"Average PESQ: {avg_PESQ:.2f}\")\n",
    "\n",
    "    return sep_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:24.795053Z",
     "iopub.status.busy": "2025-04-03T14:32:24.794773Z",
     "iopub.status.idle": "2025-04-03T14:32:24.808554Z",
     "shell.execute_reply": "2025-04-03T14:32:24.807774Z",
     "shell.execute_reply.started": "2025-04-03T14:32:24.795025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGET_SR_16 = 16000\n",
    "TARGET_SR_8 = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speakers: 68\n"
     ]
    }
   ],
   "source": [
    "speaker_dict = load_voxceleb_metadata(VOXCELEB2_TXT_BASE_DIR, VOXCELEB2_AUDIO_BASE_DIR)\n",
    "all_speakers = sorted(list(speaker_dict.keys()))\n",
    "print(f\"Total speakers: {len(all_speakers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = all_speakers[:30]\n",
    "test_ids = all_speakers[30:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:42:31.218066Z",
     "iopub.status.busy": "2025-04-03T14:42:31.217768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training mixtures...\n",
      "Creating testing mixtures...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training mixtures...\")\n",
    "train_mixtures = create_scenario(speaker_dict, train_ids, num_mixtures=100, target_sr=TARGET_SR_16)\n",
    "print(\"Creating testing mixtures...\")\n",
    "test_mixtures = create_scenario(speaker_dict, test_ids, num_mixtures=100, target_sr=TARGET_SR_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training mixtures: 100\n",
      "Number of testing mixtures: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training mixtures: {len(train_mixtures)}\")\n",
    "print(f\"Number of testing mixtures: {len(test_mixtures)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:05.845317Z",
     "iopub.status.busy": "2025-04-03T14:35:05.844993Z",
     "iopub.status.idle": "2025-04-03T14:35:05.851534Z",
     "shell.execute_reply": "2025-04-03T14:35:05.850707Z",
     "shell.execute_reply.started": "2025-04-03T14:35:05.845285Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id00812',\n",
       " 'id00817',\n",
       " 'id01066',\n",
       " 'id01106',\n",
       " 'id01298',\n",
       " 'id01460',\n",
       " 'id01509',\n",
       " 'id01593',\n",
       " 'id01618',\n",
       " 'id01822']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:05.852400Z",
     "iopub.status.busy": "2025-04-03T14:35:05.852178Z",
     "iopub.status.idle": "2025-04-03T14:35:07.855800Z",
     "shell.execute_reply": "2025-04-03T14:35:07.855094Z",
     "shell.execute_reply.started": "2025-04-03T14:35:05.852382Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-wsj02mix' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-wsj02mix' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wsj02mix' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wsj02mix' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wsj02mix' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: masknet, encoder, decoder\n",
      "/home/gr1/himanshu/voxCeleb/sr_venv/lib/python3.8/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "sepformer_model = SepformerSeparation.from_hparams(\"speechbrain/sepformer-wsj02mix\",run_opts={\"device\":\"cuda\", \"precision\": \"16-mixed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:07.857083Z",
     "iopub.status.busy": "2025-04-03T14:35:07.856775Z",
     "iopub.status.idle": "2025-04-03T14:35:10.468955Z",
     "shell.execute_reply": "2025-04-03T14:35:10.467689Z",
     "shell.execute_reply.started": "2025-04-03T14:35:07.857051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pretrained_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.470636Z",
     "iopub.status.busy": "2025-04-03T14:35:10.470339Z",
     "iopub.status.idle": "2025-04-03T14:35:10.726529Z",
     "shell.execute_reply": "2025-04-03T14:35:10.725579Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.470604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.727820Z",
     "iopub.status.busy": "2025-04-03T14:35:10.727557Z",
     "iopub.status.idle": "2025-04-03T14:35:10.744539Z",
     "shell.execute_reply": "2025-04-03T14:35:10.743891Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.727799Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WavLMModel(\n",
       "  (feature_extractor): WavLMFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): WavLMGroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): WavLMFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): WavLMEncoder(\n",
       "    (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): WavLMSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): WavLMEncoderLayer(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          (rel_attn_embed): Embedding(320, 12)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-11): 11 x WavLMEncoderLayer(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sepformer_model.to(device)\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.745532Z",
     "iopub.status.busy": "2025-04-03T14:35:10.745335Z",
     "iopub.status.idle": "2025-04-03T14:35:10.750059Z",
     "shell.execute_reply": "2025-04-03T14:35:10.749262Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.745515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_speakers = len(train_ids)\n",
    "num_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.751191Z",
     "iopub.status.busy": "2025-04-03T14:35:10.750938Z",
     "iopub.status.idle": "2025-04-03T14:35:10.763119Z",
     "shell.execute_reply": "2025-04-03T14:35:10.762256Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.751163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = pretrained_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-03T14:39:39.713051Z",
     "iopub.status.busy": "2025-04-03T14:39:39.712728Z",
     "iopub.status.idle": "2025-04-03T14:39:39.733449Z",
     "shell.execute_reply": "2025-04-03T14:39:39.732627Z",
     "shell.execute_reply.started": "2025-04-03T14:39:39.713023Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointSeparationIdentification(\n",
       "  (sepformer): SepformerSeparation(\n",
       "    (mods): ModuleDict(\n",
       "      (encoder): Encoder(\n",
       "        (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(8,), bias=False)\n",
       "      )\n",
       "      (decoder): Decoder(256, 1, kernel_size=(16,), stride=(8,), bias=False)\n",
       "      (masknet): Dual_Path_Model(\n",
       "        (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
       "        (conv1d): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (dual_mdl): ModuleList(\n",
       "          (0-1): 2 x Dual_Computation_Block(\n",
       "            (intra_mdl): SBTransformerBlock(\n",
       "              (mdl): TransformerEncoder(\n",
       "                (layers): ModuleList(\n",
       "                  (0-7): 8 x TransformerEncoderLayer(\n",
       "                    (self_att): MultiheadAttention(\n",
       "                      (att): MultiheadAttention(\n",
       "                        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                    (pos_ffn): PositionalwiseFeedForward(\n",
       "                      (ffn): Sequential(\n",
       "                        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                        (1): ReLU()\n",
       "                        (2): Dropout(p=0, inplace=False)\n",
       "                        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                    (norm1): LayerNorm(\n",
       "                      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "                    )\n",
       "                    (norm2): LayerNorm(\n",
       "                      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "                    )\n",
       "                    (dropout1): Dropout(p=0, inplace=False)\n",
       "                    (dropout2): Dropout(p=0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm): LayerNorm(\n",
       "                  (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (pos_enc): PositionalEncoding()\n",
       "            )\n",
       "            (inter_mdl): SBTransformerBlock(\n",
       "              (mdl): TransformerEncoder(\n",
       "                (layers): ModuleList(\n",
       "                  (0-7): 8 x TransformerEncoderLayer(\n",
       "                    (self_att): MultiheadAttention(\n",
       "                      (att): MultiheadAttention(\n",
       "                        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                    (pos_ffn): PositionalwiseFeedForward(\n",
       "                      (ffn): Sequential(\n",
       "                        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                        (1): ReLU()\n",
       "                        (2): Dropout(p=0, inplace=False)\n",
       "                        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                    (norm1): LayerNorm(\n",
       "                      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "                    )\n",
       "                    (norm2): LayerNorm(\n",
       "                      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "                    )\n",
       "                    (dropout1): Dropout(p=0, inplace=False)\n",
       "                    (dropout2): Dropout(p=0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm): LayerNorm(\n",
       "                  (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (pos_enc): PositionalEncoding()\n",
       "            )\n",
       "            (intra_norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
       "            (inter_norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (end_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "        (activation): ReLU()\n",
       "        (output): Sequential(\n",
       "          (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (1): Tanh()\n",
       "        )\n",
       "        (output_gate): Sequential(\n",
       "          (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (id_model): WavLMModel(\n",
       "    (feature_extractor): WavLMFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): WavLMGroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): WavLMFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): WavLMEncoder(\n",
       "      (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): WavLMSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): WavLMEncoderLayer(\n",
       "          (attention): WavLMAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "            (rel_attn_embed): Embedding(320, 12)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): WavLMFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-11): 11 x WavLMEncoderLayer(\n",
       "          (attention): WavLMAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): WavLMFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=30, bias=True)\n",
       "  (resampler): Resample()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_model = JointSeparationIdentification(sepformer_model, pretrained_model, num_speakers, embedding_dim, id_loss_weight=0.5)\n",
    "joint_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.805134Z",
     "iopub.status.busy": "2025-04-03T14:35:10.804848Z",
     "iopub.status.idle": "2025-04-03T14:35:10.808503Z",
     "shell.execute_reply": "2025-04-03T14:35:10.807839Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.805105Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.809648Z",
     "iopub.status.busy": "2025-04-03T14:35:10.809374Z",
     "iopub.status.idle": "2025-04-03T14:35:10.823647Z",
     "shell.execute_reply": "2025-04-03T14:35:10.822802Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.809616Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    mixtures, references, labels = zip(*batch)\n",
    "    \n",
    "    max_len_mix = max(m.shape[1] for m in mixtures)\n",
    "    max_len_refs = max(r.shape[1] for r in references)\n",
    "    max_len = max(max_len_mix, max_len_refs)\n",
    "    \n",
    "    padded_mixtures = []\n",
    "    for mix in mixtures:\n",
    "        pad_size = max_len - mix.shape[1]\n",
    "        if pad_size > 0:\n",
    "            pad_tensor = torch.zeros(mix.shape[0], pad_size)\n",
    "            mix_padded = torch.cat([mix, pad_tensor], dim=1)\n",
    "        else:\n",
    "            mix_padded = mix\n",
    "        padded_mixtures.append(mix_padded)\n",
    "    \n",
    "    padded_references = []\n",
    "    for refs in references:\n",
    "        pad_size = max_len - refs.shape[1]\n",
    "        if pad_size > 0:\n",
    "            pad_tensor = torch.zeros(refs.shape[0], pad_size)\n",
    "            refs_padded = torch.cat([refs, pad_tensor], dim=1)\n",
    "        else:\n",
    "            refs_padded = refs\n",
    "        padded_references.append(refs_padded)\n",
    "    \n",
    "    mixtures_batch = torch.stack(padded_mixtures)     \n",
    "    references_batch = torch.stack(padded_references)    \n",
    "    labels_batch = torch.stack(labels)                   \n",
    "    \n",
    "    return mixtures_batch, references_batch, labels_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.824692Z",
     "iopub.status.busy": "2025-04-03T14:35:10.824463Z",
     "iopub.status.idle": "2025-04-03T14:35:10.841960Z",
     "shell.execute_reply": "2025-04-03T14:35:10.841283Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.824673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MixtureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mixtures, target_sr):\n",
    "        self.mixtures = mixtures\n",
    "        self.target_sr = target_sr\n",
    "    def __len__(self):\n",
    "        return len(self.mixtures)\n",
    "    def __getitem__(self, idx):\n",
    "        mixture, refs, spk_ids = self.mixtures[idx]\n",
    "        if self.target_sr != 8000:\n",
    "            resampler = T.Resample(self.target_sr, 8000)\n",
    "            mixture = resampler(mixture)\n",
    "        refs_8k = []\n",
    "        for r in refs:\n",
    "            if self.target_sr != 8000:\n",
    "                r_ds = T.Resample(self.target_sr, 8000)(r)\n",
    "            else:\n",
    "                r_ds = r\n",
    "            upsampler = T.Resample(8000, 16000)\n",
    "            r_up = upsampler(r_ds)  \n",
    "            refs_8k.append(r_up.squeeze(0))\n",
    "        refs_tensor = torch.stack(refs_8k, dim=0)\n",
    "        spk_to_idx = {spk: i for i, spk in enumerate(sorted(set(spk_ids)))}\n",
    "        labels = torch.tensor([spk_to_idx[spk] for spk in spk_ids], dtype=torch.long)\n",
    "        return mixture, refs_tensor, labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.842995Z",
     "iopub.status.busy": "2025-04-03T14:35:10.842731Z",
     "iopub.status.idle": "2025-04-03T14:35:10.860389Z",
     "shell.execute_reply": "2025-04-03T14:35:10.859680Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.842975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MixtureDataset(train_mixtures, target_sr=TARGET_SR_16)\n",
    "test_dataset = MixtureDataset(test_mixtures, target_sr=TARGET_SR_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.861451Z",
     "iopub.status.busy": "2025-04-03T14:35:10.861184Z",
     "iopub.status.idle": "2025-04-03T14:35:10.874310Z",
     "shell.execute_reply": "2025-04-03T14:35:10.873633Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.861419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 100\n",
      "Test Dataset Size: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.875355Z",
     "iopub.status.busy": "2025-04-03T14:35:10.875067Z",
     "iopub.status.idle": "2025-04-03T14:35:10.888941Z",
     "shell.execute_reply": "2025-04-03T14:35:10.888299Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.875323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    print(len(batch))  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:10.890123Z",
     "iopub.status.busy": "2025-04-03T14:35:10.889840Z",
     "iopub.status.idle": "2025-04-03T14:35:11.003789Z",
     "shell.execute_reply": "2025-04-03T14:35:11.002946Z",
     "shell.execute_reply.started": "2025-04-03T14:35:10.890092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtures shape: torch.Size([1, 1, 155648])\n",
      "References shape: torch.Size([1, 2, 155648])\n",
      "Labels shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "mixtures_batch, references_batch, labels_batch = batch\n",
    "print(\"Mixtures shape:\", mixtures_batch.shape)        \n",
    "print(\"References shape:\", references_batch.shape)      \n",
    "print(\"Labels shape:\", labels_batch.shape)              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:11.005113Z",
     "iopub.status.busy": "2025-04-03T14:35:11.004860Z",
     "iopub.status.idle": "2025-04-03T14:35:11.009588Z",
     "shell.execute_reply": "2025-04-03T14:35:11.008810Z",
     "shell.execute_reply.started": "2025-04-03T14:35:11.005092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 100\n",
      "Number of testing batches: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:36.627034Z",
     "iopub.status.busy": "2025-04-03T14:35:36.626598Z",
     "iopub.status.idle": "2025-04-03T14:35:36.635031Z",
     "shell.execute_reply": "2025-04-03T14:35:36.634017Z",
     "shell.execute_reply.started": "2025-04-03T14:35:36.626999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(joint_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:38:12.427284Z",
     "iopub.status.busy": "2025-04-03T14:38:12.426933Z",
     "iopub.status.idle": "2025-04-03T14:38:12.431391Z",
     "shell.execute_reply": "2025-04-03T14:38:12.430415Z",
     "shell.execute_reply.started": "2025-04-03T14:38:12.427256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3520595/3612293734.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:39:44.771544Z",
     "iopub.status.busy": "2025-04-03T14:39:44.771191Z",
     "iopub.status.idle": "2025-04-03T14:41:34.640996Z",
     "shell.execute_reply": "2025-04-03T14:41:34.639617Z",
     "shell.execute_reply.started": "2025-04-03T14:39:44.771518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting joint training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_3520595/1623481857.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|██████████| 100/100 [00:55<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 325.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 287.3443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 281.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 280.1805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 279.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 279.5151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 279.2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 278.4450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 280.7335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 100/100 [00:53<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 278.8762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting joint training...\")\n",
    "train_joint_model(joint_model, train_loader, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-03T14:35:11.320444Z",
     "iopub.status.idle": "2025-04-03T14:35:11.320693Z",
     "shell.execute_reply": "2025-04-03T14:35:11.320586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating joint model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_3520595/4143839824.py:5: FutureWarning: mir_eval.separation.bss_eval_sources\n",
      "\tDeprecated as of mir_eval version 0.8.\n",
      "\tIt will be removed in mir_eval version 0.9.\n",
      "  SDR, SIR, SAR, _ = bss_eval_sources(ref_sources, est_sources)\n",
      "Evaluating: 100%|██████████| 100/100 [09:27<00:00,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Separation Metrics ---\n",
      "Average SDR: -22.26 dB\n",
      "Average SIR: 1.45 dB\n",
      "Average SAR: -19.61 dB\n",
      "Average PESQ: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating joint model on test set...\")\n",
    "sep_metrics = evaluate_joint_model(joint_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7014922,
     "sourceId": 11230568,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7024971,
     "sourceId": 11243467,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "sr_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
