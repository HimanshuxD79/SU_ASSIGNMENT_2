{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a905ee8d",
   "metadata": {},
   "source": [
    "### Speaker Verification with Pretrained and Fine-Tuned Models\n",
    "The first part of this assignment focuses on assessing a pretrained speaker verification model and subsequently improving its performance through fine-tuning. This process involves selecting an appropriate model, evaluating it on a standard dataset, and adapting it to enhance its discriminative capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9486dd2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T07:53:47.616854Z",
     "iopub.status.busy": "2025-04-01T07:53:47.616537Z",
     "iopub.status.idle": "2025-04-01T07:53:47.620821Z",
     "shell.execute_reply": "2025-04-01T07:53:47.619808Z",
     "shell.execute_reply.started": "2025-04-01T07:53:47.616826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
    "from sklearn.metrics import roc_curve, accuracy_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07ab3f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T07:53:49.791970Z",
     "iopub.status.busy": "2025-04-01T07:53:49.791681Z",
     "iopub.status.idle": "2025-04-01T07:53:49.797766Z",
     "shell.execute_reply": "2025-04-01T07:53:49.796874Z",
     "shell.execute_reply.started": "2025-04-01T07:53:49.791947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9af184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:47:49.568204Z",
     "iopub.status.busy": "2025-04-01T06:47:49.567803Z",
     "iopub.status.idle": "2025-04-01T06:47:52.344644Z",
     "shell.execute_reply": "2025-04-01T06:47:52.343382Z",
     "shell.execute_reply.started": "2025-04-01T06:47:49.568169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312e712a9e194cc3945007ea7734a7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70e8e6e9494471499322143541ac427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e46b84058f41bf83b78e1b4d241988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WavLMModel(\n",
       "  (feature_extractor): WavLMFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): WavLMGroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): WavLMFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): WavLMEncoder(\n",
       "    (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): WavLMSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): WavLMEncoderLayer(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          (rel_attn_embed): Embedding(320, 12)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-11): 11 x WavLMEncoderLayer(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/wavlm-base-plus\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = WavLMModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889608c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:47:56.304195Z",
     "iopub.status.busy": "2025-04-01T06:47:56.303888Z",
     "iopub.status.idle": "2025-04-01T06:47:56.308663Z",
     "shell.execute_reply": "2025-04-01T06:47:56.307725Z",
     "shell.execute_reply.started": "2025-04-01T06:47:56.304170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio(filepath, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n",
    "    return waveform.squeeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744980c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:47:58.606412Z",
     "iopub.status.busy": "2025-04-01T06:47:58.606063Z",
     "iopub.status.idle": "2025-04-01T06:47:58.611106Z",
     "shell.execute_reply": "2025-04-01T06:47:58.610130Z",
     "shell.execute_reply.started": "2025-04-01T06:47:58.606363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(audio_tensor):\n",
    "    inputs = feature_extractor(audio_tensor, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dffd53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:48:00.605796Z",
     "iopub.status.busy": "2025-04-01T06:48:00.605495Z",
     "iopub.status.idle": "2025-04-01T06:48:00.609639Z",
     "shell.execute_reply": "2025-04-01T06:48:00.608678Z",
     "shell.execute_reply.started": "2025-04-01T06:48:00.605774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1_norm = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_norm = vec2 / np.linalg.norm(vec2)\n",
    "    return np.dot(vec1_norm, vec2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6d2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:48:02.958744Z",
     "iopub.status.busy": "2025-04-01T06:48:02.958431Z",
     "iopub.status.idle": "2025-04-01T06:48:02.963180Z",
     "shell.execute_reply": "2025-04-01T06:48:02.962351Z",
     "shell.execute_reply.started": "2025-04-01T06:48:02.958720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_eer(labels, scores):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    abs_diffs = np.abs(fnr - fpr)\n",
    "    idx_eer = np.nanargmin(abs_diffs)\n",
    "    eer = fpr[idx_eer]  \n",
    "    return eer * 100, thresholds[idx_eer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19bdf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:48:05.210869Z",
     "iopub.status.busy": "2025-04-01T06:48:05.210581Z",
     "iopub.status.idle": "2025-04-01T06:48:05.215424Z",
     "shell.execute_reply": "2025-04-01T06:48:05.214472Z",
     "shell.execute_reply.started": "2025-04-01T06:48:05.210847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_tar_at_far(labels, scores, target_far=0.01):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    valid_idxs = np.where(fpr <= target_far)[0]\n",
    "    if len(valid_idxs) == 0:\n",
    "        return 0.0\n",
    "    tar = tpr[valid_idxs[-1]]\n",
    "    return tar * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e50544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:48:07.891141Z",
     "iopub.status.busy": "2025-04-01T06:48:07.890851Z",
     "iopub.status.idle": "2025-04-01T06:48:07.895323Z",
     "shell.execute_reply": "2025-04-01T06:48:07.894346Z",
     "shell.execute_reply.started": "2025-04-01T06:48:07.891119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(labels, scores, threshold):\n",
    "    predictions = (np.array(scores) >= threshold).astype(int)\n",
    "    return accuracy_score(labels, predictions) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c407831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T07:57:24.526442Z",
     "iopub.status.busy": "2025-04-01T07:57:24.526086Z",
     "iopub.status.idle": "2025-04-01T07:57:24.530245Z",
     "shell.execute_reply": "2025-04-01T07:57:24.529243Z",
     "shell.execute_reply.started": "2025-04-01T07:57:24.526414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vox1_audio_dir = \"/kaggle/input/voxcelebdataset-su/vox1_test_wav/wav\"  \n",
    "trial_file_path = \"/kaggle/input/vox1-trialpair/vox1_trialpair.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17f8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T07:57:27.760487Z",
     "iopub.status.busy": "2025-04-01T07:57:27.760167Z",
     "iopub.status.idle": "2025-04-01T07:57:27.764254Z",
     "shell.execute_reply": "2025-04-01T07:57:27.763378Z",
     "shell.execute_reply.started": "2025-04-01T07:57:27.760460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a31791a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T07:57:29.766768Z",
     "iopub.status.busy": "2025-04-01T07:57:29.766483Z",
     "iopub.status.idle": "2025-04-01T07:57:29.770865Z",
     "shell.execute_reply": "2025-04-01T07:57:29.770031Z",
     "shell.execute_reply.started": "2025-04-01T07:57:29.766747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_for_file(filename):\n",
    "    if filename in embedding_cache:\n",
    "        return embedding_cache[filename]\n",
    "    audio_path = os.path.join(vox1_audio_dir, filename)\n",
    "    audio = load_audio(audio_path)\n",
    "    emb = extract_embedding(audio)\n",
    "    embedding_cache[filename] = emb\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5330",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-01T07:57:31.850373Z",
     "iopub.status.busy": "2025-04-01T07:57:31.850068Z",
     "iopub.status.idle": "2025-04-01T07:57:31.872277Z",
     "shell.execute_reply": "2025-04-01T07:57:31.871495Z",
     "shell.execute_reply.started": "2025-04-01T07:57:31.850348Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(trial_file_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b4be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T07:57:36.084577Z",
     "iopub.status.busy": "2025-04-01T07:57:36.084250Z",
     "iopub.status.idle": "2025-04-01T08:02:05.880866Z",
     "shell.execute_reply": "2025-04-01T08:02:05.880079Z",
     "shell.execute_reply.started": "2025-04-01T07:57:36.084550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trials: 100%|██████████| 37611/37611 [04:29<00:00, 139.41it/s] \n"
     ]
    }
   ],
   "source": [
    "trial_labels = []\n",
    "trial_scores = []\n",
    "\n",
    "\n",
    "for line in tqdm(lines, desc=\"Processing trials\"):\n",
    "    parts = line.strip().split()\n",
    "    \n",
    "    if len(parts) != 3:\n",
    "        print(f\"Skipping malformed line: {line.strip()}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        label = int(parts[0]) \n",
    "        file1 = parts[1]\n",
    "        file2 = parts[2]\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing line: {line.strip()} - {e}\")\n",
    "        continue\n",
    "    \n",
    "    emb1 = get_embedding_for_file(file1)\n",
    "    emb2 = get_embedding_for_file(file2)\n",
    "    score = cosine_similarity(emb1, emb2)\n",
    "    \n",
    "    trial_labels.append(label)\n",
    "    trial_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f448a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:06:59.031738Z",
     "iopub.status.busy": "2025-04-01T08:06:59.031422Z",
     "iopub.status.idle": "2025-04-01T08:06:59.035942Z",
     "shell.execute_reply": "2025-04-01T08:06:59.034980Z",
     "shell.execute_reply.started": "2025-04-01T08:06:59.031713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trial_labels = np.array(trial_labels)\n",
    "trial_scores = np.array(trial_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca87490-801e-43e5-97e3-579a419c55c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:07:01.468383Z",
     "iopub.status.busy": "2025-04-01T08:07:01.468048Z",
     "iopub.status.idle": "2025-04-01T08:07:01.497863Z",
     "shell.execute_reply": "2025-04-01T08:07:01.497142Z",
     "shell.execute_reply.started": "2025-04-01T08:07:01.468354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model Verification Results:\n",
      "EER: 36.73% at threshold 0.8699\n",
      "TAR@1%FAR: 7.39%\n",
      "Speaker Verification Accuracy: 63.27%\n"
     ]
    }
   ],
   "source": [
    "eer, eer_threshold = compute_eer(trial_labels, trial_scores)\n",
    "tar_at_1_far = compute_tar_at_far(trial_labels, trial_scores, target_far=0.01)\n",
    "verification_accuracy = compute_accuracy(trial_labels, trial_scores, threshold=eer_threshold)\n",
    "print(\"Pretrained Model Verification Results:\")\n",
    "print(f\"EER: {eer:.2f}% at threshold {eer_threshold:.4f}\")\n",
    "print(f\"TAR@1%FAR: {tar_at_1_far:.2f}%\")\n",
    "print(f\"Speaker Verification Accuracy: {verification_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ce01207-974d-45b7-bf06-eca7b81983e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:09:11.472161Z",
     "iopub.status.busy": "2025-04-01T08:09:11.471817Z",
     "iopub.status.idle": "2025-04-01T08:09:13.631220Z",
     "shell.execute_reply": "2025-04-01T08:09:13.630115Z",
     "shell.execute_reply.started": "2025-04-01T08:09:11.472133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 129 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c49c69b-34cb-4481-8705-025b4776b271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T06:48:31.788380Z",
     "iopub.status.busy": "2025-04-01T06:48:31.788063Z",
     "iopub.status.idle": "2025-04-01T06:48:36.038083Z",
     "shell.execute_reply": "2025-04-01T06:48:36.037155Z",
     "shell.execute_reply.started": "2025-04-01T06:48:31.788352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c04a504e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:18:09.692886Z",
     "iopub.status.busy": "2025-04-01T08:18:09.692557Z",
     "iopub.status.idle": "2025-04-01T08:18:09.697560Z",
     "shell.execute_reply": "2025-04-01T08:18:09.696581Z",
     "shell.execute_reply.started": "2025-04-01T08:18:09.692861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "from pydub import AudioSegment\n",
    "import imageio_ffmpeg as ffmpeg\n",
    "import ffmpeg\n",
    "import io\n",
    "import random\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bbc39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:18:11.686469Z",
     "iopub.status.busy": "2025-04-01T08:18:11.686126Z",
     "iopub.status.idle": "2025-04-01T08:18:11.690730Z",
     "shell.execute_reply": "2025-04-01T08:18:11.689938Z",
     "shell.execute_reply.started": "2025-04-01T08:18:11.686437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-a3d80aedbaf6>:3: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"ffmpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca30c9",
   "metadata": {},
   "source": [
    "### ArcFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b129838f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:25:22.889953Z",
     "iopub.status.busy": "2025-04-01T08:25:22.889658Z",
     "iopub.status.idle": "2025-04-01T08:25:22.896807Z",
     "shell.execute_reply": "2025-04-01T08:25:22.895836Z",
     "shell.execute_reply.started": "2025-04-01T08:25:22.889931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(torch.clamp(1.0 - torch.pow(cosine, 2), min=1e-6))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = torch.zeros(cosine.size(), device=input.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21328ca",
   "metadata": {},
   "source": [
    "### LoRA module for linear adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba6a699d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:25:24.928572Z",
     "iopub.status.busy": "2025-04-01T08:25:24.928223Z",
     "iopub.status.idle": "2025-04-01T08:25:24.934075Z",
     "shell.execute_reply": "2025-04-01T08:25:24.933286Z",
     "shell.execute_reply.started": "2025-04-01T08:25:24.928542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=4, alpha=16.0, dropout=0.1):\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.r = r\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Linear(in_features, r, bias=False)\n",
    "            self.lora_B = nn.Linear(r, out_features, bias=False)\n",
    "            self.scaling = alpha / r\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.lora_B(self.dropout(self.lora_A(x))) * self.scaling\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ef897",
   "metadata": {},
   "source": [
    "### Fine-tuning model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b3acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:25:27.614606Z",
     "iopub.status.busy": "2025-04-01T08:25:27.614272Z",
     "iopub.status.idle": "2025-04-01T08:25:27.620449Z",
     "shell.execute_reply": "2025-04-01T08:25:27.619514Z",
     "shell.execute_reply.started": "2025-04-01T08:25:27.614578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, embedding_dim, num_classes, lora_r=4, lora_alpha=16):\n",
    "        super(FineTuneModel, self).__init__()\n",
    "        self.pretrained = pretrained_model\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.lora = LoRALinear(embedding_dim, embedding_dim, r=lora_r, alpha=lora_alpha)\n",
    "        self.arcface = ArcMarginProduct(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_values, labels):\n",
    "        outputs = self.pretrained(input_values)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        adapted_embeddings = embeddings + self.lora(embeddings)\n",
    "        logits = self.arcface(adapted_embeddings, labels)\n",
    "        return logits, adapted_embeddings\n",
    "    \n",
    "    def extract_embeddings(self, input_values):\n",
    "        outputs = self.pretrained(input_values)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        adapted_embeddings = embeddings + self.lora(embeddings)\n",
    "        return adapted_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbb876",
   "metadata": {},
   "source": [
    "### Define VoxCeleb2 Dataset for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fa557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:14.010696Z",
     "iopub.status.busy": "2025-04-01T09:08:14.010307Z",
     "iopub.status.idle": "2025-04-01T09:08:14.019204Z",
     "shell.execute_reply": "2025-04-01T09:08:14.018453Z",
     "shell.execute_reply.started": "2025-04-01T09:08:14.010663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def crop_waveform(waveform, target_length):\n",
    "    num_samples = waveform.shape[-1]\n",
    "    if num_samples <= target_length:\n",
    "        return waveform\n",
    "    start = random.randint(0, num_samples - target_length)\n",
    "    return waveform[..., start:start+target_length]\n",
    "\n",
    "class VoxCeleb2Dataset(Dataset):\n",
    "    def __init__(self, root_dir, identities, feature_extractor, max_duration=3, max_samples_per_speaker=300):\n",
    "        self.samples = []\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_duration = max_duration \n",
    "        for speaker in identities:\n",
    "            speaker_files = []\n",
    "            speaker_dir = os.path.join(root_dir, speaker)\n",
    "            for subdir, dirs, files in os.walk(speaker_dir):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith((\".m4a\", \".wav\", \".mp3\")):\n",
    "                        audio_file = os.path.join(subdir, file)\n",
    "                        speaker_files.append((audio_file, speaker))\n",
    "            if len(speaker_files) > max_samples_per_speaker:\n",
    "                speaker_files = random.sample(speaker_files, max_samples_per_speaker)\n",
    "            self.samples.extend(speaker_files)\n",
    "        self.speaker2label = {speaker: idx for idx, speaker in enumerate(sorted(identities))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, speaker = self.samples[idx]\n",
    "        if not isinstance(audio_path, str):\n",
    "            print(\"DEBUG: audio_path is not a string:\", audio_path, type(audio_path))\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "        waveform = waveform.squeeze(0) \n",
    "        target_length = 16000 * self.max_duration  \n",
    "        waveform = crop_waveform(waveform, target_length)\n",
    "        label = self.speaker2label[speaker]\n",
    "        return waveform, label\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c6a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:16.724533Z",
     "iopub.status.busy": "2025-04-01T09:08:16.724070Z",
     "iopub.status.idle": "2025-04-01T09:08:16.730605Z",
     "shell.execute_reply": "2025-04-01T09:08:16.729670Z",
     "shell.execute_reply.started": "2025-04-01T09:08:16.724486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vox2_collate_fn(batch):\n",
    "        waveforms, labels = zip(*batch)\n",
    "        waveforms_np = [w.numpy() for w in waveforms]\n",
    "        inputs = feature_extractor(waveforms_np, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        return inputs.input_values, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9edfba58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:25:34.538819Z",
     "iopub.status.busy": "2025-04-01T08:25:34.538524Z",
     "iopub.status.idle": "2025-04-01T08:25:34.542653Z",
     "shell.execute_reply": "2025-04-01T08:25:34.541643Z",
     "shell.execute_reply.started": "2025-04-01T08:25:34.538796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vox2_dir = \"/kaggle/input/voxcelebdataset-su/vox2_test_aac/aac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5716895",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:20.169151Z",
     "iopub.status.busy": "2025-04-01T09:08:20.168862Z",
     "iopub.status.idle": "2025-04-01T09:08:20.350628Z",
     "shell.execute_reply": "2025-04-01T09:08:20.349779Z",
     "shell.execute_reply.started": "2025-04-01T09:08:20.169129Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "first_folder = os.path.join(vox2_dir, os.listdir(vox2_dir)[0])\n",
    "print(f\"Directory tree for {first_folder}:\\n\")\n",
    "for root, dirs, files in os.walk(first_folder):\n",
    "    level = root.replace(first_folder, \"\").count(os.sep)\n",
    "    indent = \" \" * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = \" \" * 4 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{sub_indent}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599520f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:26.405965Z",
     "iopub.status.busy": "2025-04-01T09:08:26.405670Z",
     "iopub.status.idle": "2025-04-01T09:08:26.413786Z",
     "shell.execute_reply": "2025-04-01T09:08:26.412931Z",
     "shell.execute_reply.started": "2025-04-01T09:08:26.405943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118 identities in VoxCeleb2 dataset.\n",
      "Identities: ['id00017', 'id00061', 'id00081', 'id00154', 'id00419', 'id00562', 'id00812', 'id00817', 'id00866', 'id00926', 'id01000', 'id01041', 'id01066', 'id01106', 'id01224', 'id01228', 'id01298', 'id01333', 'id01437', 'id01460', 'id01509', 'id01541', 'id01567', 'id01593', 'id01618', 'id01822', 'id01892', 'id01989', 'id02019', 'id02057', 'id02086', 'id02181', 'id02286', 'id02317', 'id02445', 'id02465', 'id02542', 'id02548', 'id02576', 'id02577', 'id02685', 'id02725', 'id02745', 'id03030', 'id03041', 'id03127', 'id03178', 'id03347', 'id03382', 'id03524', 'id03677', 'id03789', 'id03839', 'id03862', 'id03969', 'id03978', 'id03980', 'id03981', 'id04006', 'id04030', 'id04094', 'id04119', 'id04232', 'id04253', 'id04276', 'id04295', 'id04366', 'id04478', 'id04536', 'id04570', 'id04627', 'id04656', 'id04657', 'id04862', 'id04950', 'id05015', 'id05055', 'id05124', 'id05176', 'id05202', 'id05459', 'id05594', 'id05654', 'id05714', 'id05816', 'id05850', 'id05999', 'id06104', 'id06209', 'id06310', 'id06484', 'id06692', 'id06811', 'id06816', 'id06913', 'id07312', 'id07354', 'id07396', 'id07414', 'id07426', 'id07494', 'id07620', 'id07621', 'id07663', 'id07802', 'id07868', 'id07874', 'id07961', 'id08149', 'id08374', 'id08392', 'id08456', 'id08548', 'id08552', 'id08696', 'id08701', 'id08911', 'id09017']\n",
      "Using 100 identities for training.\n",
      "Training Identities: ['id00017', 'id00061', 'id00081', 'id00154', 'id00419', 'id00562', 'id00812', 'id00817', 'id00866', 'id00926', 'id01000', 'id01041', 'id01066', 'id01106', 'id01224', 'id01228', 'id01298', 'id01333', 'id01437', 'id01460', 'id01509', 'id01541', 'id01567', 'id01593', 'id01618', 'id01822', 'id01892', 'id01989', 'id02019', 'id02057', 'id02086', 'id02181', 'id02286', 'id02317', 'id02445', 'id02465', 'id02542', 'id02548', 'id02576', 'id02577', 'id02685', 'id02725', 'id02745', 'id03030', 'id03041', 'id03127', 'id03178', 'id03347', 'id03382', 'id03524', 'id03677', 'id03789', 'id03839', 'id03862', 'id03969', 'id03978', 'id03980', 'id03981', 'id04006', 'id04030', 'id04094', 'id04119', 'id04232', 'id04253', 'id04276', 'id04295', 'id04366', 'id04478', 'id04536', 'id04570', 'id04627', 'id04656', 'id04657', 'id04862', 'id04950', 'id05015', 'id05055', 'id05124', 'id05176', 'id05202', 'id05459', 'id05594', 'id05654', 'id05714', 'id05816', 'id05850', 'id05999', 'id06104', 'id06209', 'id06310', 'id06484', 'id06692', 'id06811', 'id06816', 'id06913', 'id07312', 'id07354', 'id07396', 'id07414', 'id07426']\n"
     ]
    }
   ],
   "source": [
    "all_identities = sorted(os.listdir(vox2_dir))\n",
    "print(f\"Found {len(all_identities)} identities in VoxCeleb2 dataset.\")\n",
    "print(f\"Identities: {all_identities}\")\n",
    "train_identities = all_identities[:100]\n",
    "print(f\"Using {len(train_identities)} identities for training.\")\n",
    "print(f\"Training Identities: {train_identities}\")\n",
    "num_classes = len(train_identities)\n",
    "batch_size = 32\n",
    "num_epochs = 10  \n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61bf29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:28.779040Z",
     "iopub.status.busy": "2025-04-01T09:08:28.778725Z",
     "iopub.status.idle": "2025-04-01T09:08:35.770370Z",
     "shell.execute_reply": "2025-04-01T09:08:35.769472Z",
     "shell.execute_reply.started": "2025-04-01T09:08:28.779011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train_dataset: 23186\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VoxCeleb2Dataset(vox2_dir, train_identities, feature_extractor)\n",
    "print(f\"Number of samples in train_dataset: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8ccc460c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:38.558702Z",
     "iopub.status.busy": "2025-04-01T09:08:38.558375Z",
     "iopub.status.idle": "2025-04-01T09:08:38.564770Z",
     "shell.execute_reply": "2025-04-01T09:08:38.563896Z",
     "shell.execute_reply.started": "2025-04-01T09:08:38.558678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=vox2_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92dc8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:41.907894Z",
     "iopub.status.busy": "2025-04-01T09:08:41.907600Z",
     "iopub.status.idle": "2025-04-01T09:08:41.920607Z",
     "shell.execute_reply": "2025-04-01T09:08:41.919770Z",
     "shell.execute_reply.started": "2025-04-01T09:08:41.907869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneModel(\n",
       "  (pretrained): WavLMModel(\n",
       "    (feature_extractor): WavLMFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): WavLMGroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): WavLMFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): WavLMEncoder(\n",
       "      (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): WavLMSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): WavLMEncoderLayer(\n",
       "          (attention): WavLMAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "            (rel_attn_embed): Embedding(320, 12)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): WavLMFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-11): 11 x WavLMEncoderLayer(\n",
       "          (attention): WavLMAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): WavLMFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lora): LoRALinear(\n",
       "    (lora_A): Linear(in_features=768, out_features=4, bias=False)\n",
       "    (lora_B): Linear(in_features=4, out_features=768, bias=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (arcface): ArcMarginProduct()\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = model.config.hidden_size  \n",
    "num_classes = len(train_identities)  \n",
    "finetune_model = FineTuneModel(model, embedding_dim, num_classes, lora_r=4, lora_alpha=16)\n",
    "finetune_model.to(device)\n",
    "finetune_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11a40a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:45.683549Z",
     "iopub.status.busy": "2025-04-01T09:08:45.683184Z",
     "iopub.status.idle": "2025-04-01T09:08:45.688779Z",
     "shell.execute_reply": "2025-04-01T09:08:45.688070Z",
     "shell.execute_reply.started": "2025-04-01T09:08:45.683521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7651504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T09:08:53.065224Z",
     "iopub.status.busy": "2025-04-01T09:08:53.064930Z",
     "iopub.status.idle": "2025-04-01T10:35:57.719844Z",
     "shell.execute_reply": "2025-04-01T10:35:57.718876Z",
     "shell.execute_reply.started": "2025-04-01T09:08:53.065202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 725/725 [08:45<00:00,  1.52it/s, loss=18.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 18.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 725/725 [08:46<00:00,  1.38it/s, loss=18.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch1.pt (New best loss: 18.5720)\n",
      "Epoch 1 average loss: 18.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 725/725 [08:41<00:00,  1.53it/s, loss=17.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 17.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 725/725 [08:41<00:00,  1.39it/s, loss=17.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch2.pt (New best loss: 17.6918)\n",
      "Epoch 2 average loss: 17.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 725/725 [08:42<00:00,  1.53it/s, loss=15.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 16.4117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 725/725 [08:43<00:00,  1.39it/s, loss=15.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch4.pt (New best loss: 16.4117)\n",
      "Epoch 4 average loss: 16.4117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 725/725 [08:39<00:00,  1.52it/s, loss=13.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 14.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 725/725 [08:39<00:00,  1.39it/s, loss=13.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch5.pt (New best loss: 14.5869)\n",
      "Epoch 5 average loss: 14.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 725/725 [08:42<00:00,  1.52it/s, loss=11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 11.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 725/725 [08:42<00:00,  1.39it/s, loss=11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch6.pt (New best loss: 11.0216)\n",
      "Epoch 6 average loss: 11.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 725/725 [08:43<00:00,  1.52it/s, loss=10.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 10.9942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 725/725 [08:43<00:00,  1.38it/s, loss=10.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch7.pt (New best loss: 10.9942)\n",
      "Epoch 7 average loss: 10.9942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 725/725 [08:40<00:00,  1.55it/s, loss=10.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 10.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 725/725 [08:40<00:00,  1.39it/s, loss=10.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch8.pt (New best loss: 10.8280)\n",
      "Epoch 8 average loss: 10.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 725/725 [08:39<00:00,  1.51it/s, loss=10.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 10.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 725/725 [08:39<00:00,  1.39it/s, loss=10.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch9.pt (New best loss: 10.7202)\n",
      "Epoch 9 average loss: 10.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 725/725 [08:42<00:00,  1.48it/s, loss=10.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 10.6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 725/725 [08:43<00:00,  1.39it/s, loss=10.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved at best_finetune_model_epoch10.pt (New best loss: 10.6547)\n",
      "Epoch 10 average loss: 10.6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "best_loss = float('inf') \n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for batch_idx, (input_values, labels) in enumerate(train_dataloader):\n",
    "            input_values = input_values.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = finetune_model(input_values=input_values, labels=labels)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            model_path = f\"best_finetune_model_epoch{epoch+1}.pt\"\n",
    "            torch.save(finetune_model.state_dict(), model_path)\n",
    "            print(f\" Model saved at {model_path} (New best loss: {best_loss:.4f})\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} average loss: {epoch_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0d3a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T10:36:21.984228Z",
     "iopub.status.busy": "2025-04-01T10:36:21.983928Z",
     "iopub.status.idle": "2025-04-01T10:36:21.992101Z",
     "shell.execute_reply": "2025-04-01T10:36:21.991056Z",
     "shell.execute_reply.started": "2025-04-01T10:36:21.984203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneModel(\n",
       "  (pretrained): WavLMModel(\n",
       "    (feature_extractor): WavLMFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): WavLMGroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): WavLMFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): WavLMEncoder(\n",
       "      (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): WavLMSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): WavLMEncoderLayer(\n",
       "          (attention): WavLMAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "            (rel_attn_embed): Embedding(320, 12)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): WavLMFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-11): 11 x WavLMEncoderLayer(\n",
       "          (attention): WavLMAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): WavLMFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lora): LoRALinear(\n",
       "    (lora_A): Linear(in_features=768, out_features=4, bias=False)\n",
       "    (lora_B): Linear(in_features=4, out_features=768, bias=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (arcface): ArcMarginProduct()\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe4bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T10:36:25.647234Z",
     "iopub.status.busy": "2025-04-01T10:36:25.646925Z",
     "iopub.status.idle": "2025-04-01T10:36:25.651683Z",
     "shell.execute_reply": "2025-04-01T10:36:25.650855Z",
     "shell.execute_reply.started": "2025-04-01T10:36:25.647205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_finetune_embedding(audio_tensor):\n",
    "    inputs = feature_extractor(audio_tensor, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = finetune_model.extract_embeddings(input_values)\n",
    "    return embeddings.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6448675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T10:36:28.299504Z",
     "iopub.status.busy": "2025-04-01T10:36:28.299193Z",
     "iopub.status.idle": "2025-04-01T10:36:28.312427Z",
     "shell.execute_reply": "2025-04-01T10:36:28.311457Z",
     "shell.execute_reply.started": "2025-04-01T10:36:28.299479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_cache_finetune = {}\n",
    "def get_finetune_embedding_for_file(filename):\n",
    "    if filename in embedding_cache_finetune:\n",
    "        return embedding_cache_finetune[filename]\n",
    "    audio_path = os.path.join(vox1_audio_dir, filename) \n",
    "    audio = load_audio(audio_path)\n",
    "    emb = extract_finetune_embedding(audio)\n",
    "    embedding_cache_finetune[filename] = emb\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef421e3-ff43-4bad-844c-61035ebfe312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T10:36:34.873849Z",
     "iopub.status.busy": "2025-04-01T10:36:34.873554Z",
     "iopub.status.idle": "2025-04-01T10:40:11.873987Z",
     "shell.execute_reply": "2025-04-01T10:40:11.873230Z",
     "shell.execute_reply.started": "2025-04-01T10:36:34.873827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trials: 100%|██████████| 37611/37611 [03:36<00:00, 173.33it/s] \n"
     ]
    }
   ],
   "source": [
    "trial_labels_ft = []\n",
    "trial_scores_ft = []\n",
    "\n",
    "for line in tqdm(lines, desc=\"Processing trials\"):\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) != 3:\n",
    "        print(f\"Skipping malformed line: {line.strip()}\")\n",
    "        continue\n",
    "    try:\n",
    "        label = int(parts[0])  \n",
    "        file1 = parts[1]\n",
    "        file2 = parts[2]\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing line: {line.strip()} - {e}\")\n",
    "        continue\n",
    "    emb1 = get_finetune_embedding_for_file(file1)\n",
    "    emb2 = get_finetune_embedding_for_file(file2)\n",
    "    score = cosine_similarity(emb1, emb2)\n",
    "    trial_labels_ft.append(label)\n",
    "    trial_scores_ft.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "333ed923-4049-4b85-ba15-2e8e4627cbfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T10:43:07.138122Z",
     "iopub.status.busy": "2025-04-01T10:43:07.137750Z",
     "iopub.status.idle": "2025-04-01T10:43:07.186624Z",
     "shell.execute_reply": "2025-04-01T10:43:07.185795Z",
     "shell.execute_reply.started": "2025-04-01T10:43:07.138096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-Tuned Model Verification Results:\n",
      "EER: 15.01% at threshold 0.9895\n",
      "TAR@1%FAR: 21.62%\n",
      "Speaker Verification Accuracy: 84.99%\n"
     ]
    }
   ],
   "source": [
    "eer_ft, eer_threshold_ft = compute_eer(trial_labels_ft, trial_scores_ft)\n",
    "tar_at_1_far_ft = compute_tar_at_far(trial_labels_ft, trial_scores_ft, target_far=0.01)\n",
    "verification_accuracy_ft = compute_accuracy(trial_labels_ft, trial_scores_ft, threshold=eer_threshold_ft)\n",
    "\n",
    "print(\"\\nFine-Tuned Model Verification Results:\")\n",
    "print(f\"EER: {eer_ft:.2f}% at threshold {eer_threshold_ft:.4f}\")\n",
    "print(f\"TAR@1%FAR: {tar_at_1_far_ft:.2f}%\")\n",
    "print(f\"Speaker Verification Accuracy: {verification_accuracy_ft:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7014922,
     "sourceId": 11230568,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7015253,
     "sourceId": 11231020,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
